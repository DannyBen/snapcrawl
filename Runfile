require 'fileutils'
require 'screencap'
require 'nokogiri'
require 'open-uri'
require 'pstore'

name    "SnapCrawl"
summary "Crawl a website and capture screenshots"
version "0.1.1"

trap(:INT) { abort "\nGoodbye" }

@storefile = "snapcrawl.pstore"
@store     = PStore.new(@storefile)
@done      = []

help   "Clears the links cache"
action :cache do
  say "Deleting #{@storefile}"
  File.delete @storefile
end

usage  "snap <url> [--depth <n> --age <seconds> --dir <path>]"
help   "Crawl <url> and save screenshots"
option "-d --depth <n>", "Number of levels to crawl. 2 means crawl the base URL and any page linked from it. [default: 1]"
option "-a --age <seconds>", "Number of seconds to consider a snapshot fresh. [default: 86400]"
option "--dir <path>", "Set the folder where screenshots are stored. [default: snaps]"
action :snap do |args|
  depth     = args['--depth'].to_i
  @base     = args['<url>']
  @snap_age = args['--age'].to_i # seconds
  urls      = [protocolize(@base)]
  @dir      = args['--dir']

  make_screenshot_dir @dir

  depth.times do 
    urls = crawl_and_snap urls
  end
end

def crawl_and_snap(urls)
  new_urls = []
  urls.each do |url|
    next if @done.include? url
    @done << url
    say "!txtgrn!Processing #{url}"
    snap url
    new_urls += extract_urls_from url
  end
  new_urls
end

# Take a screenshot of a URL, unless we already did so recently
def snap(url)
  file = image_path_for(url)
  if file_fresh? file
    say "  Skipping, file exists and seems fresh"
  else
    snap!(url)
  end
end

# Take a screenshot of the URL, even if file exists
def snap!(url)
  say "  Snapping..."

  f = Screencap::Fetcher.new url
  screenshot = f.fetch(
    :output => image_path_for(url),
    ## Optionals:
    :width => 1280,
    # :height => 768,
    # :div => '.header', # selector for a specific element to take screenshot of
    # :top => 0, :left => 0, :width => 100, :height => 100 # dimensions for a specific area
  )
end

def extract_urls_from(url)
  cached = nil
  @store.transaction { cached = @store[url] }
  if cached
    say "  Getting subsequent links from cache"
    return cached
  else
    return extract_urls_from! url
  end
end

def extract_urls_from!(url)
  say "  Extracting links..."
  doc = Nokogiri::HTML open url
  links = doc.css('a')
  links = normalize_links links
  @store.transaction { @store[url] = links }
  links
end

# mkdir the screenshots folder, if needed
def make_screenshot_dir(dir)
  Dir.exists? dir or FileUtils.mkdir_p dir
end

# Convert any string to a proper handle
def handelize(str)
  str.downcase.gsub /[^a-z0-9]+/, '-'
end

# Return proper image path for a URL
def image_path_for(url)
  "#{@dir}/#{handelize(url)}.png"
end

# Add protocol to a URL if neeed
def protocolize(url)
  url =~ /^http/ ? url : "http://#{url}"
end

# Return true if the file exists and is not too old
def file_fresh?(file)
  File.exist?(file) and file_age(file) < @snap_age
end

# Return file age in seconds
def file_age(file)
  (Time.now - File.stat(file).mtime).to_i
end

# Process an array of links and return a better one
def normalize_links(links)
  # Remove the #hash part from all links
  links = links.map {|link| link.attribute('href').to_s.gsub(/#.+$/, '')}

  # Make unique and remove empties
  links = links.uniq.reject {|link| link.empty?}

  # Remove links to images and other files
  extensions = "png|gif|jpg|pdf|zip"
  links = links.reject {|link| link =~ /\.(#{extensions})(\?.*)?$/}

  # Remove mailto, tel links
  beginnings = "mailto|tel"
  links = links.reject {|link| link =~ /^(#{beginnings})/}

  # Add the base domain to relative URLs
  links = links.map {|link| link =~ /^http/ ? link : "http://#{@base}#{link}"}

  # Keep only links in our base domain
  links = links.select {|link| link =~ /https?:\/\/#{@base}.*/}

  links
end